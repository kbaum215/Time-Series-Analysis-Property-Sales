---
title: "EDA_ADS506_Project"
author: "Kevin Baum"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Load in the libraries to be used
```{r}
library(readr)
library(tidyverse)
library(dplyr)
library(ggplot2)
library(lubridate)
library(zoo) 
library(mice)
library(corrplot)
library(forecast)
library(fpp2)
library(ggfortify) 
library(xts)
```

Load in the data to be used
```{r}
# Load the dataset
data <- read_csv("raw_sales.csv")
head(data)
```

Initial EDA
```{r}
# Transform the "datesold" column to ensure it is in date format
data$datesold <- as.Date(data$datesold, format = "%m/%d/%Y %H:%M")

# Create a new column "YearMonth" with the first day of each month
data <- data %>%
  mutate(YearMonth = floor_date(datesold, unit = "month"))

# Aggregate data into monthly time index
monthly_data <- data %>%
  group_by(YearMonth) %>%
  summarise(
    TotalPrice = sum(price),
    AvgBedrooms = mean(bedrooms)
  )

# Create a new column "Quarter" to represent the quarter
data <- data %>%
  mutate(Quarter = quarter(datesold, with_year = TRUE))

# Aggregate data into quarterly time index
quarterly_data <- data %>%
  group_by(Quarter) %>%
  summarise(
    TotalPrice = sum(price),
    AvgBedrooms = mean(bedrooms)
  )
```


```{r}
# View the first few rows of the aggregated data
head(monthly_data)
head(quarterly_data)
```


```{r}
# Plot Monthly Total Price Trend
ggplot(monthly_data, aes(x = YearMonth, y = TotalPrice)) +
  geom_line() +
  labs(
    title = "Monthly Total Price Trend",
    x = "Year-Month",
    y = "Total Price"
  ) +
  theme_minimal()

# Check for missing values in monthly_data
sum(is.na(monthly_data$TotalPrice))
sum(is.na(monthly_data$YearMonth))


monthly_data$YearMonth <- zoo::na.locf(monthly_data$YearMonth)

# Plot Quarterly Total Price Trend
ggplot(quarterly_data, aes(x = Quarter, y = TotalPrice)) +
  geom_line() +
  labs(
    title = "Quarterly Total Price Trend",
    x = "Quarter",
    y = "Total Price"
  ) +
  theme_minimal()

# Additional Analysis and Visualization

# Example: Histogram of property types
ggplot(data, aes(x = propertyType)) +
  geom_bar() +
  labs(
    title = "Distribution of Property Types",
    x = "Property Type",
    y = "Count"
  ) +
  theme_minimal()

# Create a box-and-whisker plot for the "price" column
ggplot(data, aes(y = price)) +
  geom_boxplot() +
  labs(
    title = "Box-and-Whisker Plot of Price",
    y = "Price"
  ) +
  theme_minimal()

# EDA

# Highlight potential outliers
# Calculate the lower and upper bounds for potential outliers
q1 <- quantile(data$price, 0.25)
q3 <- quantile(data$price, 0.75)
iqr <- q3 - q1
lower_bound <- q1 - 1.5 * iqr
upper_bound <- q3 + 1.5 * iqr

# Identify potential outliers
outliers <- data[data$price < lower_bound | data$price > upper_bound,]

# Print the identified outliers
cat("Identified Outliers:\n")
print(outliers)
```


```{r}
# Create a missing data pattern plot
md.pattern(data)

# Summary statistics for "price"
summary_price <- summary(data$price)
mean_price <- mean(data$price)
median_price <- median(data$price)
min_price <- min(data$price)
max_price <- max(data$price)
sd_price <- sd(data$price)

# Summary statistics for "bedrooms"
summary_bedrooms <- summary(data$bedrooms)
mean_bedrooms <- mean(data$bedrooms)
median_bedrooms <- median(data$bedrooms)
min_bedrooms <- min(data$bedrooms)
max_bedrooms <- max(data$bedrooms)
sd_bedrooms <- sd(data$bedrooms)

# Create a bar plot to show the distribution of sales by "postcode"
postcode_counts <- table(data$postcode)

ggplot(data = as.data.frame(postcode_counts), aes(x = Var1, y = Freq)) +
  geom_bar(stat = "identity") +
  labs(
    title = "Distribution of Sales by Postcode",
    x = "Postcode",
    y = "Number of Sales"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Create scatterplots to explore location-specific trends
ggplot(data, aes(x = postcode, y = price)) +
  geom_point() +
  labs(
    title = "Price vs. Postcode",
    x = "Postcode",
    y = "Price"
  ) +
  theme_minimal()

ggplot(data, aes(x = postcode, y = bedrooms)) +
  geom_point() +
  labs(
    title = "Bedrooms vs. Postcode",
    x = "Postcode",
    y = "Bedrooms"
  ) +
  theme_minimal()

# Calculate the Pearson correlation between "price" and "bedrooms"
correlation_price_bedrooms <- cor(data$price, data$bedrooms)

# Print the correlation coefficient
cat("Pearson's Correlation between Price and Bedrooms: ", correlation_price_bedrooms, "\n")

# Select the variables for correlation plot
selected_vars <- c("price", "bedrooms")

# Calculate the correlation matrix for these variables
correlation_matrix <- cor(data[, selected_vars])

# create correlation plot
corrplot(correlation_matrix, method = "color", tl.cex = 0.8, tl.col = "black")

# Decomposing a time series into its components, to include random, seasonal, trend and observed, can provide insights into the underlying patterns. Here we use a monthly frequency.
# Create a time series object
ts_data <- ts(data$price, frequency = 12)  # Set frequency to 12 for monthly data

# Decompose the time series
decomposed_ts <- decompose(ts_data)

# Plot the decomposed components
plot(decomposed_ts)

```


```{r}
summary(data)
```

```{r}
str(data)
```

```{r}
sum(is.na(data))
```


```{r}
ggplot(data, aes(x=data$propertyType)) + geom_bar(fill="sky blue")
```

```{r}
ggplot(data, aes(x=factor(bedrooms))) + 
  geom_bar(fill = "lightgreen") + 
  scale_x_discrete(breaks = levels(factor(data$bedrooms)))
```

```{r}
ggplot(data, aes(x = price)) +
  geom_histogram(binwidth = diff(range(data$price)) / 30, fill = "blue", color = "black") +
  theme_minimal() +
  labs(title = "Distribution of data Prices", x = "Price", y = "Count")

```

```{r}
ggplot(data, aes(y = price)) +
  geom_boxplot(fill = "orange") +
  theme_minimal() +
  labs(title = "Boxplot of data Prices", y = "Price")

```

```{r}
#check for missing data and data inspection
sum(is.na(data))
str(data)
summary(data)
```

```{r}
#converting to proper time frame
data$datesold <- as.POSIXct(data$datesold, format = "%Y-%m-%d %H:%M:%S")

property_count <- table(data$propertyType)
```

```{r}
#time analysis
data<- data[order(data$datesold), ]

#time series plot
data$Month <- format(data$datesold, format = "%Y-%m")
monthly_counts <- aggregate(data$postcode, by = list(data$Month), FUN = length)
monthly_amounts <- aggregate(data$price, by = list(data$Month), FUN = sum)
```

```{r}
# Data Visualization
ggplot(data, aes(x = price)) +
  geom_histogram(binwidth = 50000, fill = "lightblue", color = "black") +
  labs(title = "Price Distribution", x = "Price") +
  theme_minimal()

ggplot(data, aes(x = propertyType, y = price)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Price Distribution by Property Type", x = "Property Type", y = "Price") +
  theme_minimal()
```
```{r}
corr <- cor(data[, c("price", "bedrooms")])
corr

corrplot(corr, method = "color", type = "upper", tl.col = "black")
```
```{r}
# detecting outliers
price_q1 <- quantile(data$price, 0.25)
price_q3 <- quantile(data$price, 0.75)
iqr <- price_q3 - price_q1
lower_bound <- price_q1 - 1.5 * iqr
upper_bound <- price_q3 + 1.5 * iqr
outliers <- data[data$price < lower_bound | data$price > upper_bound, ]

ggplot(data, aes(x = "datesold", y = price)) +
  geom_boxplot(fill = "lightblue") +
  geom_point(data = data[data$price < lower_bound | data$price > upper_bound, ],
             aes(x = 1, y = price), color = "red", shape = 18) +
  labs(title = "Boxplot of Price with Outliers", x = "", y = "Price") +
  theme_minimal() +
  scale_x_discrete()
```
```{r}
# Property Type Analysis
property_type_prices <- aggregate(data$price, by = list(data$propertyType), FUN = mean)
property_type_bedrooms <- aggregate(data$bedrooms, by = list(data$propertyType), FUN = mean)

print(property_type_bedrooms)
print(property_type_prices)
```
```{r}
# Bar plot for price
ggplot(property_type_prices, aes(x = Group.1, y = x)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Average Price by Property Type", x = "Property Type", y = "Average Price") +
  theme_minimal()

# Bar plot for bedrooms
ggplot(property_type_bedrooms, aes(x = Group.1, y = x)) +
  geom_bar(stat = "identity", fill = "lightblue") +
  labs(title = "Average Bedrooms by Property Type", x = "Property Type", y = "Average Bedrooms") +
  theme_minimal()
```

```{r}
data_ts <- xts(data$price, order.by = data$datesold)

# Create a time series plot using autoplot
autoplot(data_ts) +
  ggtitle("Time Series of Transactions") +
  xlab("Date") +
  ylab("Number of Transactions")
```

Check for missing values:
```{r}
colSums(is.na(data))
```


```{r}
# Data Visualization
ggplot(data, aes(x = price)) +
  geom_histogram(binwidth = 50000, fill = "lightblue", color = "black") +
  labs(title = "Price Distribution", x = "Price") +
  theme_minimal()

ggplot(data, aes(x = propertyType, y = price)) +
  geom_boxplot(fill = "lightblue") +
  labs(title = "Price Distribution by Property Type", x = "Property Type", y = "Price") +
  theme_minimal()
```

Below, we filter out the outliers based on interquartile ranges using the standard principle of 1.5 times the lowest and highest quartile. We then display the number of rows with outliers to see how many there are:
```{r}
Q1 <- quantile(data$price, 0.25)
Q3 <- quantile(data$price, 0.75)
IQR <- IQR(data$price)
outliers <- subset(data, price < (Q1 - 1.5 * IQR) | price > (Q3 + 1.5 * IQR))
nrow(outliers)
```

Below: we show the total amount of rows:
```{r}
nrow(data)
```

Next, we remove outliers:
```{r}
clean_data <- subset(data, price >= (Q1 - 1.5 * IQR) & price <= (Q3 + 1.5 * IQR))
nrow(clean_data)
```

Next we show the boxplots of prices after removing outliers
```{r}
ggplot(clean_data, aes(x = propertyType, y = price)) + 
  geom_boxplot(fill = "lightblue") +
  labs(title = "Price Distribution by Property Type After Removing Outliers", x = "Property Type", y = "Price") +
  theme_minimal()
```


```{r}
# Convert dates to year-month format

# Convert dates to year-month format
data <- data %>%
  mutate(datesold = floor_date(as.Date(datesold), "month"))
```


```{r}
# Aggregate data to monthly level
monthly_data <- data %>%
  group_by(datesold) %>%
  summarize(price = mean(price)) # or use another aggregation function as needed

head(monthly_data)
```


```{r}
# Find the latest date to get the split point
latest_date <- max(monthly_data$datesold, na.rm = TRUE)

# Display the latest date
latest_date
```




Next, we perform data splitting
```{r}
# Splitting the data into training and testing sets
set.seed(123) 

# Determine the split point to get 12 months of data before the latest date
split_date <- latest_date - 365

# Perform the split
train_data <- filter(monthly_data, datesold <= split_date)
test_data <- filter(monthly_data, datesold > split_date)

# Displaying the number of rows in training and testing
nrow(train_data)
nrow(test_data)
```


Smoothing: The point of smoothing is to smooth out short term fluctuations and highlight longer-term trends. There are different smoothing methods, and the one we use is "moving average." The window_size variable is the number of periods that the smoothing average will consider and is set to 10 as a default. A smaller value for window_size will follow the data more closely but retain more noise while a larger value will be smoother and eliminate more noise but follow the data less closely.

The Moving Average smoothing method is chosen for its simplicity, interpretability, and common use case. It is easy to understand by users that new data will affect what is already being seen but should be understood as part of the average trend when thinking holistically about patterns in a time series. One downside to be aware of is lag time. If we were talking about a moving average of a stock series, for instance, and took a 5 year trend in April of 2020, we would not be accurately capturing what was happening in the stock market during the start of the COVID19 pandemic when the S and P 500 dropped 30% in a month. 
```{r}
# Smoothing using a Moving Average
window_size <- 10  
monthly_data <- monthly_data %>%
  mutate(price_smoothed = zoo::rollapply(price, width = window_size, FUN = mean, fill = NA, align = "center"))
```


Differencing: The point of differencing is to transform the price column into a series where each value is the difference from its previous value. This makes the series more stationary.
```{r}
# Differencing the series
monthly_data <- monthly_data %>%
  mutate(price_diff = c(NA, diff(price)))
```


```{r}
# Plotting
# Original Price
ggplot(monthly_data, aes(x = datesold, y = price)) +
  geom_line() +
  labs(title = "Original Price", x = "Date", y = "Price")

# Smoothed Price
ggplot(monthly_data, aes(x = datesold, y = price_smoothed)) +
  geom_line(color = "blue") +
  labs(title = "Smoothed Price (Moving Average)", x = "Date", y = "Smoothed Price")

# Differenced Price
ggplot(monthly_data, aes(x = datesold, y = price_diff)) +
  geom_line(color = "red") +
  labs(title = "Differenced Price", x = "Date", y = "Differenced Price")
```


```{r}
start_year <- year(min(train_data$datesold))
start_month <- month(min(train_data$datesold))

```


Setting up Holt-Winters Model
```{r}
# Convert training data to a time series object
# train_ts <- ts(train_data$price, frequency = 12)
# Convert training data to a time series object
train_ts <- ts(train_data$price, start=c(start_year, start_month), frequency=12)


# Apply Holt-Winters Exponential Smoothing
hw_model <- HoltWinters(train_ts)

# Summary of the model
summary(hw_model)
```


```{r}
# Check model diagnostics for HW Model
plot(hw_model)
```


```{r}
# Forecast using the Holt-Winters model
hw_forecast <- forecast(hw_model, h = 12)

# Plot the forecast
plot(hw_forecast)
```


```{r}
end_train_year <- year(max(train_data$datesold))
end_train_month <- month(max(train_data$datesold))
```


```{r}
# Adjust for rolling over to the next year
if (end_train_month == 12) {
  start_test_year <- end_train_year + 1
  start_test_month <- 1
} else {
  start_test_year <- end_train_year
  start_test_month <- end_train_month + 1
}

# Convert test data to a time series object
test_ts <- ts(test_data$price, start=c(start_test_year, start_test_month), frequency=12)
```

```{r}
hw_forecast
```

```{r}
test_ts
```


```{r}
print(hw_forecast)
print(test_ts)
```


```{r}
# Calculate accuracy metrics
accuracy(hw_forecast, test_ts)
```




ARIMA Model
```{r}
#converting to proper time frame
data$datesold <- as.POSIXct(data$datesold, format = "%Y-%m-%d %H:%M:%S")

property_count <- table(data$propertyType)
```

```{r}
#time analysis
data<- data[order(data$datesold), ]

#time series plot
data$Month <- format(data$datesold, format = "%Y-%m")
monthly_counts <- aggregate(data$postcode, by = list(data$Month), FUN = length)
monthly_amounts <- aggregate(data$price, by = list(data$Month), FUN = sum)
```

```{r}
# detecting outliers
price_q1 <- quantile(data$price, 0.25)
price_q3 <- quantile(data$price, 0.75)
iqr <- price_q3 - price_q1
lower_bound <- price_q1 - 1.5 * iqr
upper_bound <- price_q3 + 1.5 * iqr
outliers <- data[data$price < lower_bound | data$price > upper_bound, ]

ggplot(data, aes(x = "datesold", y = price)) +
  geom_boxplot(fill = "lightblue") +
  geom_point(data = data[data$price < lower_bound | data$price > upper_bound, ],
             aes(x = 1, y = price), color = "red", shape = 18) +
  labs(title = "Boxplot of Price with Outliers", x = "", y = "Price") +
  theme_minimal() +
  scale_x_discrete()
```

```{r}
# Property Type Analysis
property_type_prices <- aggregate(data$price, by = list(data$propertyType), FUN = mean)
property_type_bedrooms <- aggregate(data$bedrooms, by = list(data$propertyType), FUN = mean)

print(property_type_bedrooms)
print(property_type_prices)
```

```{r}
data_ts <- xts(data$price, order.by = data$datesold)

autoplot(data_ts) +
  ggtitle("Time Series of Transactions") +
  xlab("Date") +
  ylab("Number of Transactions")
```
```{r}
correlation_postal_price <- cor(data$postcode, data$price)
corr_post_bed <- cor(data$bedrooms, data$postcode)
# Print the correlation result
cat("Correlation between PostalCode and Price: ", correlation_postal_price, "\n")
cat("Correlation between Postalcode and Bedroom: ", corr_post_bed, "\n")
```




SARIMA Model
```{r}
# Transform the "datesold" column to ensure it is in date format
data$datesold <- as.Date(data$datesold, format = "%m/%d/%Y %H:%M")
```

Group the data into months
```{r}
# Create a new column "YearMonth" with the first day of each month
data <- data %>%
  mutate(YearMonth = floor_date(datesold, unit = "month"))

# Aggregate data into monthly time index
monthly_data <- data %>%
  group_by(YearMonth) %>%
  summarise(
    TotalPrice = sum(price),
    AvgBedrooms = mean(bedrooms)
  )
```

Group the data into quarters
```{r}
# Create a new column "Quarter" to represent the quarter
data <- data %>%
  mutate(Quarter = quarter(datesold, with_year = TRUE))

# Aggregate data into quarterly time index
quarterly_data <- data %>%
  group_by(Quarter) %>%
  summarise(
    TotalPrice = sum(price),
    AvgBedrooms = mean(bedrooms)
  )
```

```{r}
# View the first few rows of the aggregated data
head(monthly_data)
head(quarterly_data)
```
```{r}
# Plot Monthly Total Price Trend
ggplot(monthly_data, aes(x = YearMonth, y = TotalPrice)) +
  geom_line() +
  labs(
    title = "Monthly Total Price Trend",
    x = "Year-Month",
    y = "Total Price"
  ) +
  theme_minimal()

```
```{r}
# Check for missing values in monthly_data
sum(is.na(monthly_data$TotalPrice))
sum(is.na(monthly_data$YearMonth))
```
```{r}
library(zoo)  # For the na.locf function

monthly_data$YearMonth <- zoo::na.locf(monthly_data$YearMonth)

# Plot Quarterly Total Price Trend
ggplot(quarterly_data, aes(x = Quarter, y = TotalPrice)) +
  geom_line() +
  labs(
    title = "Quarterly Total Price Trend",
    x = "Quarter",
    y = "Total Price"
  ) +
  theme_minimal()

```

Load in the data
```{r}
library(readr)
library(caret)
library(tidyverse)
```

Check for missing values:
```{r}
colSums(is.na(data))
```

Below, we filter out the outliers based on interquartile ranges using the standard principle of 1.5 times the lowest and highest quartile. We then display the number of rows with outliers to see how many there are:
```{r}
Q1 <- quantile(data$price, 0.25)
Q3 <- quantile(data$price, 0.75)
IQR <- IQR(data$price)
outliers <- subset(data, price < (Q1 - 1.5 * IQR) | price > (Q3 + 1.5 * IQR))
nrow(outliers)
```

Below: we show the total amount of rows:
```{r}
nrow(data)
```

Next, we remove outliers:
```{r}
clean_data <- subset(data, price >= (Q1 - 1.5 * IQR) & price <= (Q3 + 1.5 * IQR))
nrow(clean_data)
```


Next, we perform data splitting with an 80/20 approach
```{r}
# Splitting the data into training and testing sets
set.seed(123)  # Setting seed for reproducibility
splitIndex <- createDataPartition(clean_data$price, p = 0.8, list = FALSE)
# Creating training and testing datasets
train_data <- clean_data[splitIndex, ]
test_data <- clean_data[-splitIndex, ]
# Displaying the number of rows in training and testing
nrow(train_data)
nrow(test_data)
```

CREATING MODEL: Seasonal Autoregressive Integrated Moving Average (SAMIMA) model

```{r}
library(forecast)
```

Convert train_data to Time Series: Ensure your data is in a time series format.

```{r}
ts_data <- ts(train_data$price, frequency = 12) # Monthly data with a frequency of 12
```

Fit SARIMA Model using the auto.arima() function

```{r}
sarima_model <- auto.arima(ts_data)
```

Model Summary to see the chosen parameters and diagnostic information

```{r}
summary(sarima_model)
```
Forecasting to generate future forecasts using SARIMA model

```{r}
forecasted_values <- forecast(sarima_model, h = 12) #forecasting 12 steps ahead
```

Plotting the forecast to visualize the forecasted values and confidence intervals using the plot() funcation

```{r}
plot(forecasted_values)
```
The SARIMA model's performance using the train_data seems reasonable, however the RMSE being high suggests that the model might not be providing accurate predictions. Therefore testing using the test_data would be wise.

Convert test_data to Time Series: Ensure your data is in a time series format.

```{r}
ts_data <- ts(test_data$price, frequency = 12) # Monthly data with a frequency of 12
```

Fit SARIMA Model using the auto.arima() function
```{r}
sarima_model <- auto.arima(ts_data)
```

Model Summary to see the chosen parameters and diagnostic information

```{r}
summary(sarima_model)
```

Forecasting to generate future forecasts using SARIMA model

```{r}
forecasted_values <- forecast(sarima_model, h = 12) #forecasting 12 steps ahead
```

Plotting the forecast to visualize the forecasted values and confidence intervals using the plot() funcation

```{r}
plot(forecasted_values)
```

Running the data for the test_data, shows the same issue with the RMSE being high at 157320.2. The RMSE being high shows that the accuracy of the predictions is not trustworthy.





